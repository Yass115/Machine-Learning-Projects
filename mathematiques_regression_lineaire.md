# Explications MathÃ©matique de la rÃ©gression LinÃ©aire

---

# ğŸ¯ **Objectif**

Comprendre prÃ©cisÃ©ment :

* comment scikit-learn trouve **w** (le coefficient)
* comment il trouve **b** (lâ€™intercept)
* pourquoi le modÃ¨le trouve *la meilleure droite possible*
* comment les maths garantissent un rÃ©sultat optimal

---

# ğŸ§  1) Le modÃ¨le que scikit-learn apprend

Pour une seule variable (ton cas) :

```
prix_prÃ©dit = w Ã— surface + b
```

Scikit-learn doit trouver **le meilleur w** et **le meilleur b**
pour que la droite colle au mieux aux points.

---

# ğŸ”¥ 2) Le principe mathÃ©matique utilisÃ© : **MÃ©thode des moindres carrÃ©s**

Scikit-Learn utilise une mÃ©thode mathÃ©matique trÃ¨s classique :

> **Minimiser la somme des erreurs au carrÃ©.**

Pour chaque point, lâ€™erreur est :

```
erreur = prix_rÃ©el â€“ prix_prÃ©dit
```

Mais au lieu de prendre directement lâ€™erreur, on prend :

```
erreurÂ²
```

Pourquoi ?

* toujours positif
* punit fort les grosses erreurs
* permet de trouver une solution exacte

La somme totale des erreurs carrÃ©es est :

```
E = âˆ‘ (prix_rÃ©el â€“ (w Ã— surface + b))Â²
```

**Le but est de trouver w et b qui minimisent E.**

---

# ğŸ§± 3) Comment trouver w et b qui minimisent lâ€™erreur ?

Câ€™est ici que les maths arrivent.

Lâ€™idÃ©e :

1. On calcule la dÃ©rivÃ©e de lâ€™erreur **par rapport Ã  w**
2. On la met = 0
3. On rÃ©sout â†’ on obtient w optimal

Puis on fait pareil pour **b**.

Scikit-learn utilise cette solution **exacte**, car câ€™est plus rapide et plus stable que la descente de gradient dans les petits modÃ¨les linÃ©aires.

---

# ğŸ§© 4) Formules finales (simplement expliquÃ©es)

### âœ”ï¸ Le poids w

Pour **une seule variable**, scikit-learn utilise :

[
w = \frac{\sum (x - \bar{x})(y - \bar{y})}{\sum (x - \bar{x})^2}
]

OÃ¹ :

* (x) = surface
* (y) = prix
* (\bar{x}) = moyenne des surfaces
* (\bar{y}) = moyenne des prix

ğŸ‘‰ Câ€™est exactement la formule de la **pente dâ€™une droite de rÃ©gression**.

**InterprÃ©tation :**

> w est proportionnel Ã  la corrÃ©lation entre surface et prix.

---

### âœ”ï¸ Le biais b

Une fois w trouvÃ© :

[
b = \bar{y} - w\bar{x}
]

**InterprÃ©tation :**

> b ajuste la droite pour quâ€™elle passe â€œau bon endroitâ€.

---

# ğŸ” 5) Intuition trÃ¨s simple derriÃ¨re ces formules

ğŸ‘‰ **w** mesure combien le prix change quand la surface change.

Si les surfaces augmentent et les prix augmentent â†’ w positif
Si les surfaces augmentent mais les prix diminuent â†’ w nÃ©gatif
Si aucune relation â†’ w â‰ˆ 0

ğŸ‘‰ **b** ajuste la droite verticalement pour quâ€™elle sâ€™aligne sur les donnÃ©es.

---

# ğŸ§  6) Exemple concret avec ton dataset

Surfaces = `[30, 50, 70, 80, 120, 150, 200]`
Prix = `[80000, 120000, 160000, 180000, 250000, 310000, 400000]`

Scikit-learn va calculer :

1. la moyenne des surfaces
2. la moyenne des prix
3. les diffÃ©rences `(x - mean(x))`
4. les produits `(x - mean(x))(y - mean(y))`
5. w = somme(produits) / somme((x - mean(x))Â²)
6. b = mean(y) â€“ w Ã— mean(x)

RÃ©sultat :
Tu obtiendras un w â‰ˆ **2000** et un b â‰ˆ **20000** (approximativement).

---

# ğŸ§¨ 7) Pour plusieurs caractÃ©ristiques ?

Si on avait :

* surface
* chambres
* Ã¢ge
* etc.

Alors scikit-learn utilise la formule matricielle :

[
w = (X^T X)^{-1} X^T y
]

Câ€™est la solution analytique des moindres carrÃ©s (mÃ©thode OLS).

Ne tâ€™inquiÃ¨te pas : scikit-learn fait les calculs automatiquement.
Tu nâ€™as jamais besoin de le faire Ã  la main.

---

# ğŸ§© 8) RÃ©sumÃ© clair

| Ã‰lÃ©ment       | Comment scikit-learn le calcule ?             | Sens                                          |
| ------------- | --------------------------------------------- | --------------------------------------------- |
| **w (poids)** | Formule des moindres carrÃ©s                   | Importance de la surface                      |
| **b (biais)** | CalculÃ© Ã  partir de w et des moyennes         | Ajustement vertical                           |
| **Objectif**  | Minimiser la somme des erreurs au carrÃ©       | Trouver la meilleure droite                   |
| **MÃ©thode**   | Solution analytique sans descente de gradient | Plus rapide et exact pour les modÃ¨les simples |

---

# ğŸ‰ Tu as compris mathÃ©matiquement comment scikit-learn apprend !

Câ€™est exactement ce qui se passe dans tous les modÃ¨les linÃ©aires.

---
